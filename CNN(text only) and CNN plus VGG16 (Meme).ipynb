{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-GI7UB4mTz6T"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpwA3cNMR8DS","outputId":"d1cf98f0-6fca-43fe-ea48-ca64cd1c13b4","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]}],"source":["# Importing all the necessary libraries\n","import keras\n","import h5py\n","from keras import optimizers\n","from keras.models import load_model\n","from keras.layers import Bidirectional\n","from Multimodal_baseline_Functions import *\n","from keras.layers.core import Reshape, Dropout\n","from keras.utils.vis_utils import plot_model\n","import os\n","import matplotlib.pyplot as plt\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling3D\n","from keras import regularizers\n","import seaborn as sns\n","import matplotlib.pyplot as plt   \n","from sklearn.metrics import confusion_matrix\n","from keras import regularizers  \n","from keras.applications.inception_v3 import InceptionV3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJZ9W1BiT3jX"},"outputs":[],"source":["from Multimodal_baseline_Functions_Copy import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3WxX8IbT50r"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdkaAHx-R8Da"},"outputs":[],"source":["# Assigning class weights\n","class_weight = {1: 1.4,\n","                0: 1.}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgMXcwdJR8Dc"},"outputs":[],"source":["GLOVE_DIR = \"./drive/MyDrive/sumaiya thaseen proj/glove.6B\"\n","EMBEDDING_DIM = 50\n","num_epochs = 7\n","step_epochs = 2\n","val_steps = 149"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjpF-ouiR8De"},"outputs":[],"source":["# Defining model with Adam optimizer\n","adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n","adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aciBDGSR8Dh"},"outputs":[],"source":["def Image_model(base_model):\n","    # Freezing all the trainable layers\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    # Creating output layer\n","    x = base_model.output\n","    # Adding pooling layer before the output\n","    x = GlobalAveragePooling2D()(x)\n","    # Adding a fully-connected layer\n","    # x = Dense(1024, activation='relu')(x)\n","    # and a logistic layer with 2 classes\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQyC1Q23R8Dj"},"outputs":[],"source":["def read_data(file_name):\n","  #Opening file\n","    with open(file_name,'r', encoding=\"utf8\") as f:\n","      #Creating empty set and dictonary for vocab and word respectively\n","        word_vocab = set() \n","        word2vector = {}\n","        #Iterating over each line of file\n","        for line in f:\n","            #Spliting lines\n","            line_ = line.strip() \n","            #Splitting words\n","            words_Vec = line_.split()            \n","            word_vocab.add(words_Vec[0])\n","            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n","    print(\"Total Words in DataSet:\",len(word_vocab))\n","    return word_vocab,word2vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FUBq_AcR8Dm"},"outputs":[],"source":["# Dividing data in test, train, validation\n","training_DF, testing_DF, validation_DF = preprocess_text(Training_path,Validation_path, Testing_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOS510ROR8Dp","outputId":"e320607f-7e48-4281-8b6e-9a5a18dbb0af"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_name</th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LJ3r8Gy.jpg.png</td>\n","      <td>official bernie sanders drinking game every ti...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>qDnIIHA.png</td>\n","      <td>228 pm wall inside nazi gas chamber dwebs demo...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1JQk5NF.png</td>\n","      <td>shit waddup bernie sanders com</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>iMMNq.png</td>\n","      <td>mitt romney worst republican country putup bar...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>jAi3iI1.png</td>\n","      <td>anonymous id duqda1io 08 05 16 fri 163248 8423...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        image_name                                           sentence  label\n","0  LJ3r8Gy.jpg.png  official bernie sanders drinking game every ti...      0\n","1      qDnIIHA.png  228 pm wall inside nazi gas chamber dwebs demo...      1\n","2      1JQk5NF.png                     shit waddup bernie sanders com      1\n","3        iMMNq.png  mitt romney worst republican country putup bar...      0\n","4      jAi3iI1.png  anonymous id duqda1io 08 05 16 fri 163248 8423...      0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["training_DF.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9Ytx0PuR8Dr"},"outputs":[],"source":["# Processing image and text for each set\n","# Creating train, test and validation image path\n","train_img_path = create_img_path(training_DF,'image_name', img_dir)\n","test_img_path = create_img_path(testing_DF,'image_name', img_dir)\n","val_img_path = create_img_path(validation_DF,'image_name', img_dir)\n","\n","# Processing the text\n","training_DF['sentence'] = training_DF['sentence'].apply(clean_text)\n","testing_DF['sentence'] = testing_DF['sentence'].apply(clean_text)\n","validation_DF['sentence'] = validation_DF['sentence'].apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQkx0LkuR8Du"},"outputs":[],"source":["# Vectorising text\n","# process the whole observation into single list\n","train_text_list=list(training_DF['sentence'])\n","test_text_list = list(testing_DF['sentence'])\n","val_text_list = list(validation_DF['sentence'])\n","\n","# Creating vectors for train, test, validation\n","tokenizer = Tokenizer(num_words=1000)\n","tokenizer.fit_on_texts(train_text_list)\n","sequences_train = tokenizer.texts_to_sequences(train_text_list)\n","sequences_test = tokenizer.texts_to_sequences(test_text_list)\n","sequences_val = tokenizer.texts_to_sequences(val_text_list)\n","\n","x_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(sequences_test, maxlen=maxlen)\n","x_val = preprocessing.sequence.pad_sequences(sequences_val, maxlen=maxlen)\n","\n","# encoding all the labels \n","y_test = testing_DF['label']\n","y_train = training_DF['label']\n","y_val = validation_DF['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuGO-VUVR8Dw"},"outputs":[],"source":["# Creating train, test, val, generator for meme\n","img_txt_gen_train = img_text_generator(train_img_path, x_train, y_train, batch_size=32)\n","img_txt_gen_test = img_text_generator(test_img_path, x_test, y_test, batch_size=1)\n","img_txt_gen_val = img_text_generator(val_img_path, x_val, y_val, batch_size=1)\n","\n","# Creating train, test, val, generator for text\n","txt_gen_train = text_generator(x_train, y_train, batch_size=32)\n","txt_gen_test = text_generator(x_test, y_test, batch_size=1)\n","txt_gen_val = text_generator(x_val, y_val, batch_size=1)\n","\n","# Creating train, test, val, generator for image\n","img_gen_train = image_generator(train_img_path, training_DF, batch_size=32)\n","img_gen_test = image_generator(test_img_path, testing_DF, batch_size=1)\n","img_gen_val = image_generator(val_img_path, validation_DF, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_IRzDKZR8Dx","outputId":"bb96a10f-b951-477f-e48e-068047e17246"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Words in DataSet: 400000\n"]}],"source":["vocab, w2v = read_data(os.path.join(GLOVE_DIR, \"glove.6B.50d.txt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DS1xjpmR8Dz"},"outputs":[],"source":["word_index = tokenizer.word_index\n","num_tokens = len(word_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jy12kM2-R8D0"},"outputs":[],"source":["#Creating embeddding weight matrix\n","embedding_matrix = np.zeros((num_tokens + 1, EMBEDDING_DIM))\n","\n","for word, i in word_index.items():\n","    embedding_vector = w2v.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJTANO3nR8D1","outputId":"2f2b60f4-ffc4-4c29-8b24-dfd6fd9b47be"},"outputs":[],"source":["#Creating embedded layer using embedded matrix as weight matrix\n","embedding_layer = Embedding(num_tokens + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCgQaBMtR8D1"},"outputs":[],"source":["from keras import regularizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msnK6iWNR8D2","outputId":"d86bb0f0-fc0a-4532-c788-75c7ad3feb5b"},"outputs":[],"source":["# Defining second LSTM\n","main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n","# Adding embedding layer\n","embedded_sequences = embedding_layer(main_input)\n","x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n","x = MaxPooling1D(5)(x)\n","x = Conv1D(128, 5, activation='relu')(x)\n","x = MaxPooling1D(5)(x)\n","x = Conv1D(128, 5, activation='relu')(x)\n","x = MaxPooling1D(5)(x)\n","lstm1 = LSTM(32, return_state=True)\n","encoder_outputs,state_h,state_c = (lstm1)(x)\n","states= [state_h,state_c]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCWBqWwqR8D3"},"outputs":[],"source":["# Defining second LSTM\n","lstm2=LSTM(32, return_sequences=True, return_state=True)\n","decoder_out,_,_=lstm2(embedded_sequences,initial_state=states)\n","lstm_out = Flatten()(decoder_out)\n","txt_out = Dense(1, activation='sigmoid')(lstm_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWTPxbz_R8D4"},"outputs":[],"source":["# Defining text model\n","txt_model = Model(inputs = [main_input], outputs=txt_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZAQJFZyR8D5","outputId":"51956269-6402-45f2-a79d-b6410e01c919"},"outputs":[],"source":["# Compile text model\n","txt_model.compile(loss='binary_crossentropy', optimizer=adam, metrics = [\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUHclqiXR8D6"},"outputs":[],"source":["# Plot text model\n","plot_model(txt_model, to_file='CNN_txt_model.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdxBBJp6R8D7","outputId":"4c647846-3e86-4113-cfb7-0235397ac070"},"outputs":[],"source":["# Training text model\n","txt_model.fit_generator(txt_gen_train, epochs = num_epochs, validation_steps = val_steps, steps_per_epoch=step_epochs, validation_data=txt_gen_val, shuffle = False, class_weight=class_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHPfvZb1R8D8","outputId":"c51fab62-a3db-46f4-b1ec-496c799d6ec8"},"outputs":[],"source":["# Saving text model\n","txt_model.save('CNN_txt_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhsiNH8LR8D9","outputId":"eca6734c-a571-42d3-9109-7fc32dbd3884"},"outputs":[],"source":["# Plotting training and validation loss \n","loss_values = txt_model.history.history['loss']\n","val_loss_values = txt_model.history.history['val_loss']\n","epochs = range(1, 7 + 1)\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzXf-2bCR8D-"},"outputs":[],"source":["# Predicting labels using text model\n","y_pred_txt = (txt_model.predict_generator(txt_gen_test,steps = 149))\n","y_pred_txt = np.round(list(itertools.chain(*y_pred_txt)))\n","y_true = y_test.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQeJq6ngR8D_","outputId":"3b8110c0-9cd0-4337-f67d-fd4ba8c010f1"},"outputs":[],"source":["# Confusion matrix\n","labels = [1,0]\n","cm = confusion_matrix(y_true, y_pred_txt, labels)\n","ax= plt.subplot()\n","sns.heatmap(cm, annot=True, ax = ax, cmap='Greens'); #annot=True to annotate cells\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(['offensive', 'non-offensive']); ax.yaxis.set_ticklabels(['offensive', 'non-offensive']);\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27XpRPYAR8EA"},"outputs":[],"source":["# Loading pre-trained image model\n","img_model = load_model('VGG16_img_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lNSnEr_R8EB"},"outputs":[],"source":["# Compiling models\n","txt_model.compile(loss='binary_crossentropy', optimizer=adam, metrics = [\"accuracy\"])\n","img_model.compile(loss='binary_crossentropy', optimizer=adam, metrics = [\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bmus-7uOR8EB"},"outputs":[],"source":["# Concatenating the output of 2 classifiers\n","con_layer = keras.layers.concatenate([txt_model.output, img_model.output])\n","out = Dense(1,activation='sigmoid')(con_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PF-sKEMzR8EB"},"outputs":[],"source":["# Defining model input and output\n","com_model = Model(inputs = [img_model.input, txt_model.input], outputs=out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vi9IQs9bR8EC"},"outputs":[],"source":["# compiling the combined model\n","com_model.compile(loss='binary_crossentropy', optimizer=adam, metrics = [\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoDJHlh7R8EC"},"outputs":[],"source":["# Plot model\n","plot_model(com_model, to_file='Two_LSTM_Inception_mul_model.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgeMyOYgR8ED","outputId":"6de9f66b-cb01-434c-feb4-52f90f2d0b80","scrolled":false},"outputs":[],"source":["# Training model\n","com_model.fit_generator(img_txt_gen_train, epochs=7, validation_steps = 149, steps_per_epoch=2, validation_data=img_txt_gen_val, shuffle=False, class_weight=class_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10XbTCfkR8ED","outputId":"ca3a5882-f3da-42a0-c103-efa6088f7e12"},"outputs":[],"source":["# saving combined model\n","com_model.save(\"Two_LSTM_Inception_mul_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4jfHj4uR8EE","outputId":"5a468562-8258-4e40-90be-a18308bd3d9c","scrolled":true},"outputs":[],"source":["# Plotting training and validation loss for combined model\n","loss_values = com_model.history.history['loss']\n","val_loss_values = com_model.history.history['val_loss']\n","epochs = range(1, num_epochs + 1)\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3ko17SNR8EF"},"outputs":[],"source":["# Predicting true labels using combined classifier\n","y_pred_com = (com_model.predict_generator(img_txt_gen_test,steps = 149))\n","y_pred_com = np.round(list(itertools.chain(*y_pred_com)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcO0B9wXR8EG","outputId":"8669d4d3-6e07-46ea-cadb-f70cf6ac1ae2"},"outputs":[],"source":["# Confusion matrix\n","labels = [1,0]\n","cm = confusion_matrix(y_true, y_pred_com, labels)\n","ax= plt.subplot()\n","sns.heatmap(cm, annot=True, ax = ax,cmap='Greens'); #annot=True to annotate cells\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(['offensive', 'non-offensive']); ax.yaxis.set_ticklabels(['offensive', 'non-offensive']);\n","# plt.figure(figsize=(5,6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbsvzcMeR8EG","outputId":"61fd39d4-ce67-4df4-ef02-e281a68b5047","scrolled":true},"outputs":[],"source":["# Plotting model training accuracies\n","plt.plot(com_model.history.epoch, com_model.history.history['acc'])\n","plt.plot(txt_model.history.epoch, txt_model.history.history['acc'])\n","plt.gca().legend(('meme model acc', 'text model acc'))\n","plt.xlabel('epoch')\n","plt.ylabel('training accuracy')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kw5mJv_gR8EH","outputId":"97167c31-4db2-43f9-e32b-eb2df2ba2ed2","scrolled":true},"outputs":[],"source":["# Plotting model validation accuracies\n","plt.plot(com_model.history.epoch, com_model.history.history['val_acc'])\n","plt.plot(txt_model.history.epoch, txt_model.history.history['val_acc'])\n","plt.gca().legend(('meme model validation acc', 'text model validation acc'))\n","plt.xlabel('epoch')\n","plt.ylabel('validaion accuracy')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCIZ9apOR8EI","outputId":"89b48f74-aea7-4b59-f2aa-52af13a03f15"},"outputs":[],"source":["# Loss and accuracy for combined model\n","com_model.evaluate_generator(img_txt_gen_test, steps=149)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0j_FNgnER8EJ","outputId":"75808ada-de4a-4434-8b42-306b746d713d"},"outputs":[],"source":["# Loss and accuracy for text model\n","txt_model.evaluate_generator(txt_gen_test, steps=149)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0w8nF_RbR8EJ"},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qP5FYRJWR8EK","outputId":"588a55ee-0acf-416a-c0b4-a8c821ca4837"},"outputs":[],"source":["# for txt\n","precision_recall_fscore_support(y_true, y_pred_txt, beta=1.0, labels=None, pos_label=1, average='binary')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZ8OL4VLR8EL","outputId":"32189a87-fca2-42fb-d79f-05c45d346ca1"},"outputs":[],"source":["# com model\n","precision_recall_fscore_support(y_true, y_pred_com, beta=1.0, labels=None, pos_label=1, average='binary')"]}],"metadata":{"colab":{"name":"CNN_text_VGG16.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
